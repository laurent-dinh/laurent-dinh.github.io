---
layout: post
title: "Mixtures From Depth"
date:   2022-01-22
blurb: "When computation reuse results in mixture models."
og_image: /assets/img/content/depth_decomp_diagram_static.svg
# draft: true
---
<style>
.figure-svg {
  width: 100%;
  margin-left: auto;
  margin-right: auto;
  display: block;
  margin-bottom: 20px;
}

</style>
{% assign red = "#8c2323" %}
{% assign blue = "#00008c" %}
{% assign green = "#238c23" %}
{% assign light_red = "#8c232390" %}
{% assign light_blue = "#00008c90" %}
{% assign light_green = "#238c2390" %}

![Mixtures from Depth](/assets/img/content/depth_decomp_diagram_static.svg)

#### Table of Contents
- [Primer on Flow-Based Models](#primer-on-flow-based-models)
- [The Trick](#the-trick)
- [Gating Network](#gating-network)
- [Limits and Extensions](#limits-and-extensions)
- [Acknowledgement](#acknowledgement)
- [References](#references)

One of the predominant approaches to deep generative modelling has been the generator network paradigm, where a deep neural network transforms a simple base distribution to approximate a target distribution. This kind of method includes <span data-toggle="popover" data-placement="bottom" data-content="see <a href='https://arxiv.org/abs/1406.2661'><em>&quot;Generative Adversarial Networks&quot;</em></a>" alt="Goodfellow et al., 2014">generative adversarial networks</span>, <span data-toggle="popover" data-placement="bottom" data-content="see <a href='https://arxiv.org/abs/1312.6114'><em>&quot;Auto-Encoding Variational Bayes&quot;</em></a>, <a href='https://arxiv.org/abs/1401.4082'><em>&quot;Stochastic Backpropagation and Approximate Inference in Deep Generative Models&quot;</em></a>, and <a href='https://arxiv.org/abs/2007.03898'><em>&quot;NVAE: A Deep Hierarchical Variational Autoencoder&quot;</em></a>" alt="Kingma & Welling, 2013; Rezende et al., 2014; Vahdat & Kautz, 2020">variational autoencoders</span>, <span data-toggle="popover" data-placement="bottom" data-content="see <a href='https://arxiv.org/abs/1410.8516'><em>&quot;NICE: Non-linear Independent Components Estimation&quot;</em></a>, <a href='https://arxiv.org/abs/1605.08803'><em>&quot;Density estimation using Real NVP&quot;</em></a>, <a href='https://arxiv.org/abs/1908.09257'><em>&quot;Normalizing Flows: An Introduction and Review of Current Methods&quot;</em></a>, and <a href='https://arxiv.org/abs/1912.02762'><em>&quot;Normalizing Flows for Probabilistic Modeling and Inference&quot;</em></a>" alt="Dinh et al., 2014; Dinh et al., 2017; Kobyzev et al., 2019; Papamakarios and Nalisnick et al., 2019">normalizing flows</span>, and <span data-toggle="popover" data-placement="bottom" data-content="see <a href='https://arxiv.org/abs/1503.03585'><em>&quot;Deep Unsupervised Learning using Nonequilibrium Thermodynamics&quot;</em></a>, <a href='https://arxiv.org/abs/2006.11239'><em>&quot;Denoising Diffusion Probabilistic Models&quot;</em></a>, and <a href='https://yang-song.github.io/blog/2021/score/'><em>&quot;Generative Modeling by Estimating Gradients of the Data Distribution&quot;</em></a>" alt="Sohl Dickstein et al., 2015; Ho et al., 2020; Song, 2021">diffusion models</span>. The go-to methods to increase the capacity of these models are to increase width/depth or to apply another architectural advance to the generator net.

Another method to increase the capacity of a generative model is through <span data-toggle="popover" data-placement="bottom" data-content="see <a href='https://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100325'><em>&quot;Finite Mixture Models&quot;</em></a>" alt="McLachlan et al., 2000">mixture modelling</span>. Mixture models increase the capacity of a probabilistic density model by using multiple instances (the mixture components) and combining them through simple convex summation.
<details markdown=1>
  <summary markdown="span">**Primer on mixture models**</summary>

  <div class="details" markdown=1>
  The idea behind mixture models is to combine multiple instances of simpler families of probability distributions to build a more expressive distribution. More formally, these instances $$p_{X \mid K}^{(\theta)}$$ (mixture components) are indexed by a discrete variable $$ \textcolor{#8c2323}{K} \in \{0, \dots, \lvert K\rvert\} $$ and the resulting distribution density is expressed as a convex sum of the component distribution densities,

  $$
  p_{X}^{(\theta)}(x) = \sum_{\textcolor{#8c2323}{k}=1}^{\lvert K\rvert}{p_{\textcolor{#8c2323}{K}}^{(\theta)}(\textcolor{#8c2323}{k}) p_{X \mid \textcolor{#8c2323}{K}}^{(\theta)}(x \mid \textcolor{#8c2323}{k})},
  $$

  where the sum of these convex coefficients $$ p_{K}^{(\theta)}(k) $$ (called mixing weights) is $$ 1 $$:

  $$
  \sum_{k=1}^{\lvert K\rvert}{p_{K}^{(\theta)}(k)} = 1.
  $$

  The generative process goes as follow: randomly sample the mixture component index $$k$$ then sample from the associated component distribution, i.e.,

  $$
  \begin{aligned}
  k &\sim p_{K}^{(\theta)} \\
  x &\sim p_{X \mid K}^{(\theta)}(\cdot \mid k).
  \end{aligned}
  $$

  </div>
</details>  
When naively <span data-toggle="popover" data-placement="bottom" data-content="e.g., <a href='https://arxiv.org/abs/2106.03135'><em>&quot;Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction&quot;</em></a> and <a href='https://easychair.org/publications/paper/Scnv'><em>&quot;Mixtures of Normalizing Flows&quot;</em></a>" alt="Postels and Liu et al., 2021; Ciobanu, 2021">applied</span> to these generator network architectures, mixture models introduce a difficult trade-off between network capacity and the number of mixture components given a fixed computation and memory budget. On the one hand, each instance of this deep generative model should be expressive enough to approximate the target distribution, but, on the other hand, the computation required for one model is multiplied by the number of mixture components. At one end of this trade-off, practitioners often opt for using more but simpler mixture components, e.g., <span data-toggle="popover" data-placement="bottom" data-content="e.g.,
<a href='https://arxiv.org/abs/1406.5298'><em>the M2 model in &quot;Semi-Supervised Learning with Deep Generative Models&quot;</em></a>,
<a href='https://github.com/RuiShu/vae-experiments/tree/master/modality'><em>Rui Shu's experiment</em></a>,
<a href='https://arxiv.org/abs/1611.02648'><em>&quot;Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders&quot;</em></a>,
<a href='https://arxiv.org/abs/1611.02648'><em>&quot;Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations on single cell data&quot;</em></a>,
<a href='https://arxiv.org/abs/1705.07120'><em>&quot;VAE with a VampPrior&quot;</em></a>,
and 
<a href='https://proceedings.mlr.press/v119/izmailov20a.html'><em>&quot;Semi-Supervised Learning with Normalizing Flows&quot;</em></a>" alt="Kingma et al., 2014; Shu, 2016; Dilokthanakul et al., 2016; Kopf et al., 2019; Tomczak and Welling, 2018; Izmailov et al., 2020">gaussians</span>, as part of a larger deep generative model.

An underexplored trick could be used in conjunction with several types of deep generative models, e.g., variational autoencoders, normalizing flows, and diffusion models, to increase their capacity with little overhead in computation or memory. This trick relies on one key principle of deep models: the reuse of previous computation.

### Primer on Flow-Based Models
<embed class="figure-svg" style="max-width: 300px;" src='{{ "flow_icon" | append: ".svg" | prepend: "/assets/img/research/" }}'/>

I'll explain the method by applying it to one family of deep generative models I'm familiar with: normalizing flow. First, let's have a short, high-level description of flow-based generative models (for a more comprehensive description, read any of these <span data-toggle="popover" data-placement="bottom" data-content="<a href='https://arxiv.org/abs/1908.09257'><em>&quot;Normalizing Flows: An Introduction and Review of Current Methods&quot;</em></a> and <a href='https://arxiv.org/abs/1912.02762'><em>&quot;Normalizing Flows for Probabilistic Modeling and Inference&quot;</em></a>" alt="Kobyzev et al., 2019; Papamakarios & Nalisnick et al., 2019">reviews</span>, <span data-toggle="popover" data-placement="bottom" data-content="
<a href='https://blog.evjang.com/2018/01/nf1.html'><em>&quot;Normalizing Flows Tutorial&quot;</em></a>,
<a href='http://akosiorek.github.io/ml/2018/04/03/norm_flows.html'><em>&quot;Normalizing Flows&quot;</em></a>,
<a href='http://ruishu.io/2018/05/19/change-of-variables/'><em>&quot;Change of Variables: A Precursor to Normalizing Flow&quot;</em></a>,
and <a href='https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html'><em>&quot;Flow-based Deep Generative Models&quot;</em></a>" alt="Jang, 2018; Kosiorek, 2018; Shu, 2018; Weng, 2018">blogposts</span>, or <span data-toggle="popover" data-placement="bottom" data-content="for example
<em>&quot;<a href='https://deepgenerativemodels.github.io/notes/flow/'>Normalizing Flow Models</a> (<a href='https://deepgenerativemodels.github.io/'>Deep Generative Models</a>)&quot;</em> or
<em>&quot;<a href='https://sites.google.com/view/berkeley-cs294-158-sp20/home#h.p_E-C2dsllTu6x'>Flow Models</a> <a href='https://sites.google.com/view/berkeley-cs294-158-sp20/home'>(Deep Unsupervised Learning)</a>&quot;</em>" alt="Grover et al., 2018; Abbeel & Chen & Ho & Srinivas, 2019">these course notes</span> or watch <span data-toggle="popover" data-placement="bottom" data-content="for example
<em>&quot;<a href='https://youtu.be/i7LjDvsLWCg'>What are Normalizing Flows?</a>&quot;</em> or
<em>&quot;<a href='/resume#invertible-models-and-normalizing-flows-a-retrospective-talk'>Invertible Models and Normalizing Flows: A Retrospective Talk</a>&quot;</em>, and
<em>&quot;<a href='https://youtu.be/8XufsgG066A'>Normalizing Flows and Invertible Neural Networks in Computer Vision</a>&quot;</em>" alt="Seff, 2019; Dinh, 2020; Brubaker, 2020">these videos</span>).

Flow-based generative models rely on a expressive, parametrized, invertible function (flow) $$f^{(\theta)}$$ with a tractable Jacobian determinant $$\left\lvert\frac{\partial f^{(\theta)}}{\partial x}\right\rvert$$. Given a simple (and often standard) base distribution $$p_Z$$, we can compute the density for the resulting deep generative model, and therefore approximately maximize log-likelihood, using [the change of variable formula](https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function):

$$
p_{X}^{(\theta)}(x) = p_Z\big(f^{(\theta)}(x)\big) \left\lvert\frac{\partial f^{(\theta)}}{\partial x}(x)\right\rvert.
$$

We can then generate from this model using the inverse $$\big(f^{(\theta)}\big)^{-1}$$ of this invertible function $$f^{(\theta)}$$ as follows:

$$
\begin{aligned}
z &\sim p_Z \\
x &= \big(f^{(\theta)}\big)^{-1}(z).
\end{aligned}
$$

### The Trick
<embed class="figure-svg" style="max-width: 500px;" src='{{ "depth_decomp_diagram" | append: ".svg" | prepend: "/assets/img/content/" }}'/>

Often, this flow $$f^{(\theta)}$$ is a composition of simpler invertible components $$\big(f_{l}^{(\theta)}\big)_{l \leq \lvert L\rvert}$$ with $$\lvert L\rvert$$ being the number of invertible layers:

$$
f^{(\theta)} = f_{\lvert L\rvert}^{(\theta)} \circ f_{\lvert L\rvert-1}^{(\theta)} \circ \dots \circ f_1^{(\theta)}.
$$

We adopt the following notations,

$$
\begin{aligned}
f_{\leq l}^{(\theta)} &= f_l^{(\theta)} \circ f_{l-1}^{(\theta)} \circ \dots \circ f_1^{(\theta)} \\
z_l &= f_{\leq l}^{(\theta)}(x) = f_l^{(\theta)}(z_{l-1}),
\end{aligned}
$$

and remember that the inverse of a composion of function is the composition of their inverses in reverse order,

$$
\big(f_{\leq l}^{(\theta)}\big)^{-1} = \big(f_1^{(\theta)}\big)^{-1} \circ \dots \circ \big(f_l^{(\theta)}\big)^{-1},
$$

and that the Jacobian determinant of a composition of function is the product of their respective Jacobian determinant,

$$
\left\lvert\frac{\partial f_{\leq l}^{(\theta)}}{\partial x}(x)\right\rvert = \left\lvert\frac{\partial f_{l}^{(\theta)}}{\partial z_{l-1}}(x)\right\rvert \dots
\left\lvert\frac{\partial f_{1}^{(\theta)}}{\partial x}(x)\right\rvert.
$$

Therefore, each partial composition $$f_{\leq l}^{(\theta)}$$ can stand on its own as a flow-based model. Interestingly, there is shared computation between 
$$f_{\leq l}^{(\theta)}$$ and $$f_{\leq l'}^{(\theta)}$$, namely if $$l < l'$$ then most of the computation used for computing the log-likelihood for $$f_{\leq l}^{(\theta)}$$ will be used for $$f_{\leq l'}^{(\theta)}$$.

With little overhead, we obtain the following tied parameters mixture model:

$$
\begin{aligned}
p_{X}^{(\theta)}(x) &= \sum_{l=1}^{\lvert L\rvert}{p_{L}^{(\theta)}(l)~p_Z\big(f_{\leq l}^{(\theta)}(x)\big) \left\lvert\frac{\partial f_{\leq l}^{(\theta)}}{\partial x}(x)\right\rvert}.
\end{aligned}
$$

As the base distribution factor is propagated at every level, the associated loss becomes very reminiscent of [deep supervision of neural nets](https://arxiv.org/abs/1409.5185) and [stochastic depth training](https://arxiv.org/abs/1603.09382).

Not only we gain a potentially stricly <span data-toggle="tooltip" data-placement="bottom" title="moreover, the fact that it's a mixture model often allow for theoretical universal approximation">more expressive family of distributions</span> without compromise but the sampling process becomes

$$
\begin{aligned}
z &\sim p_Z \\
l &\sim p_{L}^{(\theta)} \\
x &= \big(f_{\leq l}^{(\theta)}\big)^{-1}(z),
\end{aligned}
$$

which is potentially faster than sampling from the flow $$f^{(\theta)}$$. <!-- Relatedly, component collapse for this tied mixture model becomes less problematic. For example, if $$\exists l', \forall l > l', p_{l}^{(\theta)} \simeq 0$$ then the associated $$\big(f_{l}^{(\theta)}\big)_{l > l'}$$ can be pruned to obtain a more efficient model. Moreover, $$f_{l}^{(\theta)}$$ is unlikely to be a waste of parameters as long as it differs enough from the identity function. -->

### Gating Network
<embed class="figure-svg" style="max-width: 500px;" src='{{ "gating_network_figure" | append: ".svg" | prepend: "/assets/img/content/" }}'/>

As described in [*"Mixture Density Networks"*](https://publications.aston.ac.uk/id/eprint/373/), one can make the mixture weights $$p_{L}^{(\theta)}$$ a function of another variable. In particular, the observation made in [*"Locally-Connected Transformations for Deep GMMs"*](https://lib.ugent.be/en/catalog/pug01:7028865) is that these mixture weights can be a function of $$z$$ through a *gating network* $$p_{L \mid Z}^{(\theta)}$$. 

Applied to this method, this yields the density expression,

$$
\begin{aligned}
p_{X}^{(\theta)}(x) &= \sum_{l=1}^{\lvert L\rvert}{p_{L \textcolor{#8c2323}{\mid Z}}^{(\theta)}\big(l \textcolor{#8c2323}{\mid f_{\leq l}^{(\theta)}(x)}\big)~p_Z\big(f_{\leq l}^{(\theta)}(x)\big) \left\lvert\frac{\partial f_{\leq l}^{(\theta)}}{\partial x}(x)\right\rvert},
\end{aligned}
$$

and the sampling process,

$$
\begin{aligned}
z &\sim p_Z \\
l &\sim p_{L \textcolor{#8c2323}{\mid Z}}^{(\theta)}(\cdot \textcolor{#8c2323}{\mid z}) \\
x &= \big(f_{\leq l}^{(\theta)}\big)^{-1}(z),
\end{aligned}
$$

which means that the model can be trained to potentially [adapt its computation](https://arxiv.org/abs/1603.08983) to more [efficiently sample](https://arxiv.org/abs/2106.03802) from this model through a single loss function, the negative log-likelihood.

The gating network term can also be interpreted as a modifier to the base distribution $$p_Z$$ for each component, changing it to $$\propto p_{L \mid Z}^{(\theta)}(l \mid \cdot)~p_Z$$. This provides a more flexible parametrization to this new base distribution, similar to the [Learned Accept/Reject Sampling](https://arxiv.org/abs/1810.11428) or [Noise Contrastive Prior](https://arxiv.org/abs/2010.02917) approaches, providing a less wasteful sampling process (no rejection step involved) albeit with a more constrained parametrization. Note that, as a result of this gating network,

$$
p_{L, X}^{(\theta)}(l, x) = p_{L \mid Z}^{(\theta)} \big(l \mid f_{\leq l}^{(\theta)}(x)\big)~p_Z\big(f_{\leq l}^{(\theta)}(x)\big) \left\lvert\frac{\partial f_{\leq l}^{(\theta)}}{\partial x}(x)\right\rvert
$$

cannot be easily decomposed into a product $$p_{L \mid X}^{(\theta)}(x \mid x) \cdot p_{X}^{(\theta)}(x)$$ nor $$p_{X \mid L}^{(\theta)}(x \mid l) \cdot p_{L}^{(\theta)}(l)$$.

### Limits and Extensions
One could compose this method, i.e., use this mixtures from depth technique for $$p_Z$$. While this results in a multiplicative number of "modes", like a <span data-toggle="popover" data-placement="bottom" data-content="see
<a href='https://arxiv.org/abs/1206.4635'><em>&quot;Deep Mixtures of Factor Analysers&quot;</em></a>,
<a href='https://proceedings.neurips.cc/paper/2014/hash/8c3039bd5842dca3d944faab91447818-Abstract.html'><em>&quot;Factoring Variations in Natural Images with Deep Gaussian Mixture Models
&quot;</em></a>, and
<a href='https://arxiv.org/abs/1711.06929'><em>&quot;Deep Gaussian Mixture Models
&quot;</em></a>"
alt="Tang et al., 2012; van den Oord & Schrauwen, 2014; Viroli & McLachlan, 2017">deep mixture model</span>, this also results in the <span data-toggle="popover" data-placement="bottom" data-content="see for example
<a href='https://hal.archives-ouvertes.fr/hal-02985701'><em>&quot;A bumpy journey: exploring deep Gaussian mixture models&quot;</em></a>"
alt="Selosse et al., 2020">computational complications</span> that comes with it. There are however <span data-toggle="popover" data-placement="bottom" data-content="see for example
<a href='/resume#a-rad-approach-to-deep-mixture-models'><em>&quot;A RAD approach to deep mixture models&quot;</em></a> where the expressivity of the gating network becomes more critical."
alt="Dinh et al., 2019">alternatives</span> to overcome this issue.

While this method can be extended to other tractable probabilistic models such as <span data-toggle="popover" data-placement="bottom" data-content="see
<a href='https://mitpress.mit.edu/books/graphical-models-machine-learning-and-digital-communication'><em>&quot;Graphical Models for Machine Learning and Digital Communication&quot;</em></a>,
<a href='https://arxiv.org/abs/1601.06759'><em>&quot;Pixel Recurrent Neural Networks&quot;</em></a>, and
<a href='https://arxiv.org/abs/1606.05328'><em>&quot;Conditional Image Generation with PixelCNN Decoders&quot;</em></a>" alt="Frey, 1998; van den Oord et al., 2015; van den Oord et al., 2016">autoregressive models</span>, extending it to models trained through an evidence lower bound (variational autoencoders and some diffusion models) is less straightforward although possible through a modified evidence lower bound, e.g.,

$$
\log\big(p(x)\big) \geq \sum_{l=1}^{\vert L\rvert} q_{L \mid X}(l \mid x) \big(\mathcal{L}_Z(l) + \mathcal{L}_L(l)\big)
$$

where,

$$
\begin{align*}
\mathcal{L}_Z(l) &= \int q_{Z \mid L, X}(z \mid l, x) \log\left(\frac{p_{X, Z_{<L} \mid L}(x, z_{<l} \mid l)p_{Z}(z_l)}{q_{Z \mid L, X}(z \mid l, x)}\right) dz,\\
\mathcal{L}_L(l) &= \int q_{Z \mid L, X}(z \mid l, x) \log\left(\frac{p_{L \mid Z_L}(l \mid z_l)}{q_{L \mid X}(l \mid x)}\right) dz.
\end{align*}
$$


### Acknowledgement
I'd like to thank [Kyle Kastner]({{ site.data.links.kkastner_url }}), [David Warde-Farley]({{ site.data.links.dwf_url }}), [Erin Grant]({{ site.data.links.erm_grant_url }}) and [Arthur Gretton]({{ site.data.links.gretton_url }}) for encouraging discussions leading to the writing of this blogpost.

<details markdown="1" class="references">
  <summary id="references">References</summary>
- Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua (2014), [**Generative Adversarial Networks**](https://papers.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html), [*Advances in Neural Information Processing Systems&nbsp;27 (NeurIPS&nbsp;2014)*](https://papers.neurips.cc/paper/2014)
- Kingma, Durk and Welling, Max (2014), [**Auto-Encoding Variational Bayes**](https://openreview.net/forum?id=33X9fd2-9FyZd), [*2nd International Conference on Learning Representations&nbsp;2014 (Conference Track Proceedings)*](https://openreview.net/group?id=ICLR.cc/2014/conference)
- Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan (2014) [**Stochastic Backpropagation and Approximate Inference in Deep Generative Models**](https://proceedings.mlr.press/v32/rezende14.html), [*Proceedings of the 31st International Conference on Machine Learning (ICML&nbsp;2014)*](https://proceedings.mlr.press/v32/)
- Vahdat, Arash and Kautz, Jan (2020) [**NVAE: A Deep Hierarchical Variational Autoencoder**](https://proceedings.neurips.cc/paper/2020/hash/e3b21256183cf7c2c7a66be163579d37-Abstract.html), [*Advances in Neural Information Processing Systems&nbsp;33 (NeurIPS&nbsp;2020)*](https://proceedings.neurips.cc/paper/2020)
- Dinh, Laurent and Krueger, David and Bengio, Yoshua (2014) [**NICE: Non-linear Independent Components Estimation**](https://arxiv.org/abs/1410.8516), [*3rd International Conference on Learning Representations&nbsp;2015 (Workshop Track Proceedings)*]({{ site.data.links.iclr2015_url | prepend: site.data.links.iclr_url }})
- Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy (2016) [**Density estimation using Real NVP**](https://openreview.net/forum?id=HkpbnH9lx), [*5th International Conference on Learning Representations&nbsp;2017 (Conference Track Proceedings)*](https://openreview.net/group?id=ICLR.cc/2017/conference)
- Kobyzev, Ivan and Prince, Simon J.D. and  Brubaker, Marcus A. (2019) [**Normalizing Flows: An Introduction and Review of Current Methods**](https://ieeexplore.ieee.org/document/9089305), [*IEEE Transactions on Pattern Analysis and Machine&nbsp;Intelligence (Volume:&nbsp;43 Issue:&nbsp;11)*](https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9556112)
- Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji (2019) [**Normalizing Flows for Probabilistic Modeling and Inference**](https://jmlr.org/papers/v22/19-1028.html), [*Journal of Machine Learning&nbsp;Research (Volume&nbsp;22)*](https://jmlr.org/papers/v22/)
- Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya (2015) [**Deep Unsupervised Learning using Nonequilibrium Thermodynamics**](http://proceedings.mlr.press/v37/sohl-dickstein15.html), [*Proceedings of the 32nd International Conference on Machine&nbsp;Learning (ICML&nbsp;2015)*](https://proceedings.mlr.press/v37/)
- Ho, Jonathan and Jain, Ajay and Abbeel, Pieter (2020) [**Denoising Diffusion Probabilistic Models**](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html), [*Advances in Neural Information Processing Systems&nbsp;33 (NeurIPS&nbsp;2020)*](https://proceedings.neurips.cc/paper/2020)
- Song, Yang (2021) [**Generative Modeling by Estimating Gradients of the Data Distribution**](https://yang-song.github.io/blog/2021/score/), [*Generative Space*](https://yang-song.github.io/blog/)
- McLachlan, Geoffrey J. and Lee, Sharon X. and Rathnayake, Suren I. (2000) [**Finite Mixture Models**](https://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100325), [*Annual Review of Statistics and Its Application (Volume&nbsp;6)*](https://www.annualreviews.org/toc/statistics/6/1)
- Postels, Janis and Liu, Mengya and Spezialetti, Riccardo and Van Gool, Luc and Tombari, Federico (2021) [**Go with the Flows: Mixtures of Normalizing Flows for Point Cloud Generation and Reconstruction**](https://www.computer.org/csdl/proceedings-article/3dv/2021/268800b249/1zWEkPrnaSI), [*Proceedings of the 2021 International Conference on 3D&nbsp;Vision (3DV)*](https://www.computer.org/csdl/proceedings/3dv/2021/1zWE36wtuCY)
- Ciobanu, Sebastian (2021) [**Mixtures of Normalizing Flows**](https://easychair.org/publications/paper/Scnv), [Proceedings of ISCA 34th International Conference on Computer Applications in Industry and Engineering](https://easychair.org/publications/volume/CAINE_2021)
- Kingma, Durk and Rezende, Danilo Jimenez and Mohamed, Shakir and Welling, Max (2014) [**Semi-Supervised Learning with Deep Generative Models**](https://papers.neurips.cc/paper/2014/hash/d523773c6b194f37b938d340d5d02232-Abstract.html) [*Advances in Neural Information Processing Systems&nbsp;27 (NeurIPS&nbsp;2014)*](https://papers.neurips.cc/paper/2014)
- Shu, Rui (2015) [**`multi-modality`**](https://github.com/RuiShu/vae-experiments/tree/master/modality)
- Dilokthanakul, Nat and Mediano, Pedro A.M. and Garnelo, Marta and Lee, Matthew C.H. and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray (2016) [**Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders**](https://arxiv.org/abs/1611.02648)
- Kopf, Andreas and Fortuin, Vincent and Somnath, Vignesh Ram and Claassen, Manfred (2019) [**Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations on single cell data**](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009086), [*PLOS Computational Biology*](https://journals.plos.org/ploscompbiol/)
- Tomczak, Jakub and Welling, Max (2018) [**VAE with a VampPrior**](http://proceedings.mlr.press/v84/tomczak18a.html), [*Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics (AISTATS&nbsp;2018)*](http://proceedings.mlr.press/v84/)
- Izmailov, Pavel and Kirichenko, Polina and Finzi, Marc and Wilson, Andrew Gordon (2020) [**Semi-Supervised Learning with Normalizing Flows**](https://proceedings.mlr.press/v119/izmailov20a.html), [*Proceedings of the 37th International Conference on Machine&nbsp;Learning (ICML&nbsp;2020)*](https://proceedings.mlr.press/v119/)
- Jang, Eric (2018) [**Normalizing Flows Tutorial**](https://blog.evjang.com/2018/01/nf1.html)
- Kosiorek, Adam (2018) [**Normalizing Flows**](http://akosiorek.github.io/ml/2018/04/03/norm_flows.html)
- Wang, Lillian (2018) [**Flow-based Deep Generative Models**](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html), [*Lil'log*](https://lilianweng.github.io/lil-log/)
- Grover, Aditya and Song, Jiaming (2018) [**Normalizing Flow Models**](https://deepgenerativemodels.github.io/notes/flow/), [*Deep Generative Models*](https://deepgenerativemodels.github.io/)
- Abbeel, Pieter and Chen, Peter Xi and Ho, Jonathan and Srinivas, Aravind (2019) [**Flow Models**](https://sites.google.com/view/berkeley-cs294-158-sp20/home#h.p_E-C2dsllTu6x) [*Deep Unsupervised Learning*](https://sites.google.com/view/berkeley-cs294-158-sp20/home)
- Seff, Ari (2019) [**What are Normalizing Flows?**](https://youtu.be/i7LjDvsLWCg)
- Dinh, Laurent (2020) [**Invertible Models and Normalizing Flows: A Retrospective Talk**](https://iclr.cc/virtual_2020/speaker_4.html), [*Keynote at the 8th International Conference on Learning Representations (ICLR&nbsp;2020)*]({{ site.data.links.iclr2020_url | prepend: site.data.links.iclr_url }})
- Brubaker, Marcus (2020) [**Normalizing Flows and Invertible Neural Networks in Computer Vision**](https://youtu.be/8XufsgG066A), [*IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021*](https://cvpr2021.thecvf.com/)
- Wikipedia contributors (2022) [**Probability density function/Function of random variables and change of variables in the probability density function**](https://en.wikipedia.org/wiki/Probability_density_function#Function_of_random_variables_and_change_of_variables_in_the_probability_density_function), [*Wikipedia, The Free Encyclopedia*](https://en.wikipedia.org/)
- Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen (2015) [**Deeply-Supervised Nets**](http://proceedings.mlr.press/v38/lee15a.html), [*Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS&nbsp;2015)*](http://proceedings.mlr.press/v38/)
- Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q. (2016) [**Deep Networks with Stochastic Depth**](https://link.springer.com/chapter/10.1007/978-3-319-46493-0_39), [*Proceedings of the 14th European Conference on Computer Vision (ECCV&nbsp;2016)*](https://link.springer.com/book/10.1007/978-3-319-46493-0)
- Bishop, Christopher Michael (1994) [**Mixture density networks**](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)
- van den Oord, Aäron and Dambre, Joni (2015) [**Locally-connected transformations for deep GMMs**](https://lib.ugent.be/en/catalog/pug01:7028865), [*Deep learning Workshop (32nd International Conference on Machine&nbsp;Learning - ICML&nbsp;2015)*](https://sites.google.com/site/deeplearning2015/accepted-papers)
- Graves, Alex (2016) [**Adaptive Computation Time for Recurrent Neural Networks**](https://arxiv.org/abs/1603.08983)
- Watson, Daniel and Ho, Jonathan and Norouzi, Mohammad and Chan, William (2021) [**Learning to Efficiently Sample from Diffusion Probabilistic Models**](https://arxiv.org/abs/2106.03802)
- Bauer, Matthias and Mnih, Andriy (2018) [**Resampled Priors for Variational Autoencoders**](http://proceedings.mlr.press/v89/bauer19a.html), [*Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics (AISTATS 2019)*](http://proceedings.mlr.press/v89/)
- Aneja, Jyoti and Schwing, Alexander and Kautz, Jan and Vahdat, Arash (2020) [**A Contrastive Learning Approach for Training Variational Autoencoder Priors**](https://proceedings.neurips.cc/paper/2021/hash/0496604c1d80f66fbeb963c12e570a26-Abstract.html), [*Advances in Neural Information Processing Systems&nbsp;34 (NeurIPS&nbsp;2021)*](https://papers.neurips.cc/paper/2021)
- Tang, Yichuan Charlie and Salakhutdinov, Ruslan and Hinton, Geoffrey (2012) [**Deep Mixtures of Factor Analysers**](https://dl.acm.org/doi/10.5555/3042573.3042718), [*Proceedings of the 29th International Coference on International Conference on Machine Learning (ICML&nbsp;2012)*](https://dl.acm.org/doi/proceedings/10.5555/3042573)
- van den Oord, Aäron and Schrauwen, Benjamin (2014) [**Factoring Variations in Natural Images with Deep Gaussian Mixture Models**](https://proceedings.neurips.cc/paper/2014/hash/8c3039bd5842dca3d944faab91447818-Abstract.html), [*Advances in Neural Information Processing Systems&nbsp;27 (NeurIPS&nbsp;2014)*](https://papers.neurips.cc/paper/2014)
- Viroli, Cinzia and McLachlan, Geoffrey J. (2017) [**Deep Gaussian Mixture Models**](https://dl.acm.org/doi/abs/10.1007/s11222-017-9793-z), [*Statistics and Computing (Volume:&nbsp;29 Issue:&nbsp;1)*](https://dl.acm.org/toc/klu-stco/2019/29/1)
- Selosse, Margot and Gormley, Claire and Jacques, Julien and Biernacki, Christophe (2020) [**A bumpy journey: exploring deep Gaussian mixture model**](https://openreview.net/forum?id=Q6tXbLK7YYC), [*I Can't Believe It's Not Better! Workshop (NeurIPS&nbsp;2020)*](https://i-cant-believe-its-not-better.github.io/neurips2020/accepted_papers/)
- Dinh, Laurent and Sohl-Dickstein, Jascha and Larochelle, Hugo and Pascanu, Razvan (2019) [**A RAD approach to deep mixture models**](https://openreview.net/forum?id=HJeZNLIt_4), [*Deep Generative Models for Highly Structured Data Workshop (7th International Conference on Learning Representations - ICLR&nbsp;2019)*](https://deep-gen-struct.github.io/iclr2019/index.html)
- Frey, Brendan (1998) [**Graphical Models for Machine Learning and Digital Communication**](https://mitpress.mit.edu/books/graphical-models-machine-learning-and-digital-communication), [*MIT Press*](https://mitpress.mit.edu/)
- van den Oord, Aäron and Kalchbrenner, Nal and Kavukcuoglu, Koray (2015) [**Pixel Recurrent Neural Networks**](https://dl.acm.org/doi/10.5555/3045390.3045575), [*Proceedings of the 33rd International Conference on International Conference on Machine Learning (ICML&nbsp;2016)*](https://dl.acm.org/doi/proceedings/10.5555/3045390)
- van den Oord, Aäron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray (2016) [**Conditional Image Generation with PixelCNN Decoders**](https://papers.neurips.cc/paper/2016/hash/b1301141feffabac455e1f90a7de2054-Abstract.html), [*Advances in Neural Information Processing Systems&nbsp;29 (NeurIPS&nbsp;2016)*](https://papers.neurips.cc/paper/2016)
</details>


<!-- 


def get_citeable_name(in_):
  split_in = in_.split(" ")
  res = " ".join(split_in[:-1])
  return split_in[-1] + ", " + res

def get_citeable_list(in_):
  list_names = in_.split(", ")
  list_names = map(get_citeable_name, list_names)
  return " and ".join(list(list_names))


-->

### Citation
For attribution in academic contexts, please cite this work as
<pre><code>Dinh, Laurent (2022) "{{ page.title }}", <a style="color: unset;" href="{{ page.url | prepend: site.url }}">{{ site.blog_name }}</a>.
</code></pre>
BibTeX citation:
<div style="position: relative;">
<pre onclick="navigator.clipboard.writeText(removeNonASCII(this.textContent));modifyTooltip(this, 'Copied!', 500);" data-toggle="tooltip" data-placement="top" title="Copy to clipboard" style="cursor: pointer; white-space: pre; border: none;"><code>@misc{
  dinh_depth_2022,
  title={&zwnj;{{ page.title }}&zwnj;},
  url={&zwnj;{{ page.url | prepend: site.url }}&zwnj;},
  journal={&zwnj;{{ site.blog_name }}&zwnj;},
  author={Dinh, Laurent},
  year={&zwnj;{{ page.date | date: "%Y" }}&zwnj;},
  month={&zwnj;{{ page.date | date: "%b" }}&zwnj;}
}
</code></pre><div style="text-align: right; width: 18px; height: 18px; padding: 0px; margin: 0px; position: absolute; bottom:20px; right: 20px; opacity: 75%;"><i class="fas fa-clipboard"></i></div></div>